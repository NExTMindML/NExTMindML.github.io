<app-tittle>Fuentes de Errores</app-tittle>

<app-paragraph>
    En Aprendizaje Automático, y hablando solo del modelo supervisado, hay dos fuentes principales de errores al aprender de los datos: Sesgo (Bias) y Varianza. Cualquier error de aprendizaje puede caer en una de estas categorías.
</app-paragraph>

<app-paragraph>
    Además de estos errores, también existe el error irreducible que no se puede reducir independientemente del algoritmo utilizado (no está influenciado por los algoritmos de Aprendizaje Automático). A diferencia de los errores de Sesgo y Varianza que pueden ser influenciados por el algoritmo de Aprendizaje Automático utilizado.
</app-paragraph>

<app-paragraph>
    Vamos a especificar un poco más en qué consisten estos errores que pueden ser influenciados.
</app-paragraph>

<app-sub-title>Sesgo (Bias)</app-sub-title>

<app-paragraph>
    El Sesgo se refiere a las simplificaciones hechas por el algoritmo para que el problema sea fácil de resolver. Es por eso que los algoritmos paramétricos generalmente tienen un Sesgo alto, lo que los hace fáciles de aprender y simples de entender, pero menos flexibles.
</app-paragraph>

<app-paragraph>
    Pero, ¿a qué nos referimos con alto Sesgo? Bueno, decimos que hay un Sesgo alto cuando se sugieren más suposiciones sobre la forma de la función objetivo.
</app-paragraph>

<app-paragraph>
    Por el contrario, decimos que hay un Sesgo bajo cuando hay menos suposiciones sugeridas sobre la forma de la función objetivo.
</app-paragraph>

<app-sub-title>Varianza</app-sub-title>

<app-paragraph>
    La Varianza se refiere a la sensibilidad del modelo al cambio de los datos de entrenamiento. Generalmente, los algoritmos no paramétricos, que tienen mucha flexibilidad, tienen una Varianza alta.
</app-paragraph>

<app-paragraph>
    Al igual que en el Sesgo, podemos dividir este tipo de error en dos partes: alta varianza y baja varianza.
</app-paragraph>

<app-paragraph>
    Decimos que hay una baja varianza cuando hay pequeños cambios en la estimación de la función objetivo cuando cambian los datos de entrenamiento.
</app-paragraph>

<app-paragraph>
    Por otro lado, decimos que hay una alta varianza cuando hay muchos cambios en la estimación de la función objetivo cuando cambian los datos de entrenamiento. Así, como era de esperar, los algoritmos que tienen alta varianza están fuertemente influenciados por los datos de entrenamiento específicos.
</app-paragraph>

<app-sub-title>Objetivo</app-sub-title>

<app-paragraph>En términos de fuentes de error, ya que tanto el Sesgo como la Varianza pueden ser influenciados por el algoritmo específico, el objetivo de cada algoritmo (supervisado) será mantener un Sesgo bajo y una Varianza baja. Pero en la realidad sucede que:</app-paragraph>
<ul>
    <app-list-item>
        Los algoritmos paramétricos suelen tener un Sesgo alto y una Varianza baja.
    </app-list-item>

    <app-list-item>
        Los algoritmos no paramétricos suelen tener una Varianza baja pero un Sesgo alto.
    </app-list-item>
</ul>

<app-paragraph>En general, diremos que:</app-paragraph>

<ul>
    <app-list-item>
        Si aumenta el Sesgo, la Varianza disminuye.
    </app-list-item>

    <app-list-item>
        Si aumenta la Varianza, el Sesgo disminuye.
    </app-list-item>
</ul>

<app-sub-title>Resumen</app-sub-title>

<app-paragraph>En esta sección vimos que:</app-paragraph>

<ul>
    <app-list-item>
        El Sesgo son las simplificaciones hechas por el modelo realizadas por la función objetivo para facilitar su aproximación.
    </app-list-item>

    <app-list-item>
        La Varianza es la cantidad de estimaciones de la función objetivo que cambiarán dado un nuevo conjunto de datos de entrenamiento.
    </app-list-item>

    <app-list-item>
        La compensación es la tensión entre el error introducido por el Sesgo y la Varianza.
    </app-list-item>
</ul>
