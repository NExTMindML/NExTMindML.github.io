<app-tittle>Param-type Algorithms</app-tittle>

<app-paragraph>In the world of Machine Learning, it can be said that there are two main classes of algorithms: "Parametric" and "Non-Parametric".</app-paragraph>

<app-paragraph>All Machine Learning algorithms can be organized in these two groups. In this section we will try to explain what they are, and what are their advantages and considerations.</app-paragraph>

<app-sub-title>Parametric</app-sub-title>

<app-paragraph>
    These algorithms simplify the objective function to an already known assumed shape, so it involves two steps:
</app-paragraph>

<app-paragraph>
    1 - Select a shape for the function.
</app-paragraph>

<app-paragraph>
    2 - Learn the coefficients for the function from the test data.
</app-paragraph>

<app-paragraph>
    This assumption being made can greatly simplify the learning process, but it can also limit what can be learned.
</app-paragraph>

<app-paragraph>
    Among the benefits and limitations of this class of algorithms are:
</app-paragraph>

<app-mini-sub-title>Benefits</app-mini-sub-title>

<ul>
    <app-list-item>
        Simple: This method is easy to understand and interpret.
    </app-list-item>

    <app-list-item>
        Fast: They are very fast in learning from the data.
    </app-list-item>

    <app-list-item>
        Less data: They do not require a large amount of training data and can work well even if some of the data is not perfect.
    </app-list-item>
</ul>

<app-mini-sub-title>Limitations</app-mini-sub-title>

<ul>
    <app-list-item>
        Restricted: By choosing a functional form this method is largely limited to a specific form.
    </app-list-item>

    <app-list-item>
        Limited complexity: These methods are best suited for simple problems.
    </app-list-item>

    <app-list-item>
        Poor Fit: In practice, the methods are unlikely to match the underlying mapping function.<br>This is a major problem that lies in the assumption made, since the function to be known may not be of the selected form, leading to unsatisfactory results.
    </app-list-item>
</ul>

<app-mini-sub-title>Examples</app-mini-sub-title>

<app-paragraph>Some examples of parametric algorithms are:</app-paragraph>

<ul>
    <app-list-item>
        Logistic Regression
    </app-list-item>

    <app-list-item>
        Linear Discriminant Analysis
    </app-list-item>

    <app-list-item>
        Linear Regression
    </app-list-item>

    <app-list-item>
        Perceptron
    </app-list-item>
</ul>

<app-callout>Note: Algorithms that assume the function mapping, in a linear form, are often referred to as "Linear Machine Learning Algorithms".</app-callout>

<app-sub-title>Non-Parametric</app-sub-title>

<app-paragraph>
    Unlike parametric algorithms, non-parametric algorithms do not make strong assumptions about the shape of the objective function. Therefore, they can learn any mapping from inputs to outputs.
</app-paragraph>

<app-paragraph>
    Because of their characteristics, these types of algorithms are good when you have a lot of data and no prior knowledge, as well as when you are not too concerned about choosing only the right features.
</app-paragraph>

<app-paragraph>
    Among the benefits and limitations of non-parametric algorithms are the following:
</app-paragraph>

<app-mini-sub-title>Benefits</app-mini-sub-title>

<ul>
    <app-list-item>
        Flexibility: They are capable of adjusting to a large number of functional forms.
    </app-list-item>

    <app-list-item>
        Power: No (or very low) assumptions about the underlying function.
    </app-list-item>

    <app-list-item>
        Performance: Can result in highly performing models for prediction.
    </app-list-item>
</ul>

<app-mini-sub-title>Limitations</app-mini-sub-title>

<ul>
    <app-list-item>
        More data: Requires a lot of data for training to estimate the objective function.
    </app-list-item>

    <app-list-item>
        Slow: Much slower to train since it has more parameters to train on.
    </app-list-item>

    <app-list-item>
        Overfitting: More risk of overfitting the training data and difficult to explain why specific predictions were made.
    </app-list-item>
</ul>

<app-mini-sub-title>Examples:</app-mini-sub-title>

<app-paragraph>Some examples of non-parametric algorithms are:</app-paragraph>

<ul>
    <app-list-item>
        Naive Bayes
    </app-list-item>

    <app-list-item>
        Decision Trees such as CART and C4.5
    </app-list-item>

    <app-list-item>
        Support Vector Machines
    </app-list-item>

    <app-list-item>
        Neural Networks
    </app-list-item>
</ul>

<app-sub-title>Summary</app-sub-title>

<ul>
    <app-list-item>
        Parametric methods make large assumptions about the mapping of input variables to output variables, which makes it fast to train and requires less data, but may not be very powerful.
    </app-list-item>

    <app-list-item>
        Non-Parametric methods make few or no assumptions about the objective function, which makes it require a lot of data, makes it much slower to train, and with higher complexity, but can result in more powerful models.
    </app-list-item>
</ul>
<app-tittle>Algoritmos Paramétricos</app-tittle>

<app-paragraph>En el mundo del Aprendizaje Automático, se puede decir que hay dos clases principales de algoritmos: "Paramétricos" y "No Paramétricos".</app-paragraph>

<app-paragraph>Todos los algoritmos de Aprendizaje Automático se pueden organizar en estos dos grupos. En esta sección intentaremos explicar qué son y cuáles son sus ventajas y consideraciones.</app-paragraph>

<app-sub-title>Paramétricos</app-sub-title>

<app-paragraph>Estos algoritmos simplifican la función objetivo a una forma asumida ya conocida, lo que implica dos pasos:</app-paragraph>

<app-paragraph>
    1 - Seleccionar una forma para la función.
</app-paragraph>

<app-paragraph>
    2 - Aprender los coeficientes para la función a partir de los datos de prueba.
</app-paragraph>

<app-paragraph>
    Esta suposición puede simplificar en gran medida el proceso de aprendizaje, pero también puede limitar lo que se puede aprender.
</app-paragraph>

<app-paragraph>
    Entre los beneficios y limitaciones de esta clase de algoritmos se encuentran:
</app-paragraph>

<app-mini-sub-title>Beneficios</app-mini-sub-title>

<ul>
    <app-list-item>
        Simpleza: Este método es fácil de entender e interpretar.
    </app-list-item>

    <app-list-item>
        Rapidez: Son muy rápidos en aprender de los datos.
    </app-list-item>

    <app-list-item>
        Menos datos: No requieren una gran cantidad de datos de entrenamiento y pueden funcionar bien incluso si algunos datos no son perfectos.
    </app-list-item>
</ul>

<app-mini-sub-title>Limitaciones</app-mini-sub-title>

<ul>
    <app-list-item>
        Restringidos: Al elegir una forma funcional, este método está en gran medida limitado a una forma específica.
    </app-list-item>

    <app-list-item>
        Complejidad limitada: Estos métodos son más adecuados para problemas simples.
    </app-list-item>

    <app-list-item>
        Ajuste deficiente: En la práctica, es poco probable que los métodos coincidan con la función de mapeo subyacente. <br>Este es un problema importante que radica en la suposición realizada, ya que la función a conocer puede no tener la forma seleccionada, lo que lleva a resultados insatisfactorios.
    </app-list-item>
</ul>

<app-mini-sub-title>Ejemplos</app-mini-sub-title>

<app-paragraph>Algunos ejemplos de algoritmos paramétricos son:</app-paragraph>

<ul>
    <app-list-item>
        Regresión Logística
    </app-list-item>

    <app-list-item>
        Análisis Discriminante Lineal
    </app-list-item>

    <app-list-item>
        Regresión Lineal
    </app-list-item>

    <app-list-item>
        Perceptrón
    </app-list-item>
</ul>

<app-callout>Nota: Los algoritmos que asumen la función de mapeo en una forma lineal a menudo se denominan "Algoritmos Lineales de Aprendizaje Automático".</app-callout>

<app-sub-title>No Paramétricos</app-sub-title>

<app-paragraph>
    A diferencia de los algoritmos paramétricos, los algoritmos no paramétricos no hacen suposiciones fuertes sobre la forma de la función objetivo. Por lo tanto, pueden aprender cualquier mapeo de entradas a salidas.
</app-paragraph>

<app-paragraph>
    Debido a sus características, estos tipos de algoritmos son buenos cuando se tiene mucha data y no hay conocimientos previos, así como cuando no se está demasiado preocupado por elegir solo las características correctas.
</app-paragraph>

<app-paragraph>
    Entre los beneficios y limitaciones de los algoritmos no paramétricos se encuentran los siguientes:
</app-paragraph>

<app-mini-sub-title>Beneficios</app-mini-sub-title>

<ul>
    <app-list-item>
        Flexibilidad: Son capaces de ajustarse a una gran cantidad de formas funcionales.
    </app-list-item>

    <app-list-item>
        Poder: No (o muy bajo) hace suposiciones sobre la función subyacente.
    </app-list-item>

    <app-list-item>
        Rendimiento: Puede dar lugar a modelos altamente eficientes para la predicción.
    </app-list-item>
</ul>

<app-mini-sub-title>Limitaciones</app-mini-sub-title>

<ul>
    <app-list-item>
        Más datos: Requiere mucha data para entrenar y estimar la función objetivo.
    </app-list-item>

    <app-list-item>
        Lento: Mucho más lento de entrenar ya que tiene más parámetros para entrenar.
    </app-list-item>

    <app-list-item>
        Sobreadaptación: Mayor riesgo de sobreajustar los datos de entrenamiento y difícil de explicar por qué se hicieron predicciones específicas.
    </app-list-item>
</ul>

<app-mini-sub-title>Ejemplos:</app-mini-sub-title>

<app-paragraph>Algunos ejemplos de algoritmos no paramétricos son:</app-paragraph>

<ul>
    <app-list-item>
        Naive Bayes
    </app-list-item>

    <app-list-item>
        Árboles de Decisión como CART y C4.5
    </app-list-item>

    <app-list-item>
        Máquinas de Soporte Vectorial
    </app-list-item>

    <app-list-item>
        Redes Neuronales
    </app-list-item>
</ul>

<app-sub-title>Resumen</app-sub-title>

<ul>
    <app-list-item>
        Los métodos paramétricos hacen grandes suposiciones sobre el mapeo de variables de entrada a variables de salida, lo que lo hace rápido de entrenar y requiere menos datos, pero puede no ser muy potente.
    </app-list-item>

    <app-list-item>
        Los métodos no paramétricos hacen pocas o ninguna suposición sobre la función objetivo, lo que hace que requiera mucha data, sea mucho más lento de entrenar y tenga una mayor complejidad, pero puede dar lugar a modelos más potentes.
    </app-list-item>
</ul>
